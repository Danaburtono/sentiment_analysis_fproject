{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04a9ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e818b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d7c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the test database\n",
    "twitter_df = pd.read_csv(\"../../res/initial_dataset.csv\")\n",
    "twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee7222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the database\n",
    "def preprocess_tweet(tweet):\n",
    "    '''Cleans text data up, leaving only 2 or more char long non-stepwords composed of A-Z & a-z only\n",
    "    in lowercase'''\n",
    "    \n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # Remove RT\n",
    "    sentence = re.sub('RT @\\w+: ',\" \", tweet)\n",
    "\n",
    "    # Remove special characters\n",
    "    tweet = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", tweet)\n",
    "\n",
    "    # Single character removal\n",
    "    tweet = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', tweet)  # When we remove apostrophe from the word \"Mark's\", the apostrophe is replaced by an empty space. Hence, we are left with single character \"s\" that we are removing here.\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)  # Next, we remove all the single characters and replace it by a space which creates multiple spaces in our text. Finally, we remove the multiple spaces from our text as well.\n",
    "\n",
    "    # Remove URL's\n",
    "    tweet = re.sub('((www.[^s]+)|(https?://[^s]+))',' ',tweet)\n",
    "    \n",
    "    #Replace 2a|2nd amendment to second amendment\n",
    "    tweet = re.sub(\"2a|2nd\\samendment|2nd|2ndamendment|secondamendment|2ndamendment\", 'second amendment', tweet)\n",
    "    \n",
    "    # Remove numbers\n",
    "    tweet = re.sub('[0-9]+', '', tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1278812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned tweets in new cleaned column\n",
    "cleaned_tweets = []\n",
    "\n",
    "for tweet in twitter_df['full_text']:\n",
    "  cleaned_tweet = preprocess_tweet(tweet)\n",
    "  cleaned_tweets.append(cleaned_tweet)\n",
    "\n",
    "twitter_df['cleaned'] = pd.DataFrame(cleaned_tweets)\n",
    "twitter_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e323026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column text\n",
    "twitter_df = twitter_df.drop(['Unnamed: 0','tweet_id', 'full_text'], axis=1)\n",
    "twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de68bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwordlist = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a9901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and removing the above stop words list from the tweet text\n",
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "twitter_df['cleaned'] = twitter_df['cleaned'].apply(lambda text: cleaning_stopwords(text))\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75368d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting tokenization of tweet text\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "twitter_df['cleaned'] = twitter_df['cleaned'].apply(tokenizer.tokenize)\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd31d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Stemming\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "twitter_df['cleaned'] = twitter_df['cleaned'].apply(lambda x: stemming_on_text(x))\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f835bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Lemmatizer\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "twitter_df['cleaned'] = twitter_df['cleaned'].apply(lambda x: lemmatizer_on_text(x))\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df128d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing words with less frequency\n",
    "# filter function to select only the words with more than 10 counts and less than 800.\n",
    "import itertools\n",
    "flat_list = list(itertools.chain.from_iterable(twitter_df['cleaned']))\n",
    "\n",
    "fd = nltk.FreqDist(flat_list)\n",
    "word_to_keep = list(filter(lambda x: 800>x[1]>10, fd.items()))\n",
    "\n",
    "word_list_to_keep= [item[0] for item in word_to_keep]\n",
    "\n",
    "def remove_lessfreq(tokanized_tweets):\n",
    "    text_out = [word for word in tokanized_tweets if word in word_list_to_keep]\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['cleaned'] = twitter_df['cleaned'].apply(lambda x: remove_lessfreq(x))\n",
    "twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd47c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating input feature and label\n",
    "X=twitter_df.cleaned\n",
    "y=twitter_df.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c374b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our dataset into Train and Test Subset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data in single line through passing clean_text in the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df = 5, ngram_range=(1,5)) \n",
    "countVector = vectorizer.fit_transform(X_train.apply(lambda x: ' '.join(x)))\n",
    "print(countVector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff29bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(X_train.apply(lambda x: ' '.join(x)))\n",
    "X_test  = vectorizer.transform(X_test.apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36fb5a1",
   "metadata": {},
   "source": [
    "## Balanced Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the training data with the BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "brf_model = BalancedRandomForestClassifier(n_estimators =130)\n",
    "brf_model.fit(X_train, y_train)\n",
    "y_pred = brf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated the balanced accuracy score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced9afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486cbd93",
   "metadata": {},
   "source": [
    "## Predicting Big Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f0553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the test database\n",
    "big_twitter_df= pd.read_csv(\"../dana/big_data_tweets.csv\")\n",
    "big_twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c77a7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop column dummy sentiment\n",
    "big_twitter_df = big_twitter_df.drop(['dummy_sentiment'], axis=1)\n",
    "big_twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641758c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_twitter_df = big_twitter_df.dropna(subset=['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233591da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned tweets in new cleaned column\n",
    "big_cleaned_tweets = []\n",
    "\n",
    "for tweet in big_twitter_df['full_text']:\n",
    "    cleaned_tweet = preprocess_tweet(tweet)\n",
    "    cleaned_tweets.append(cleaned_tweet)\n",
    "\n",
    "big_twitter_df['cleaned'] = pd.DataFrame(cleaned_tweets)\n",
    "big_twitter_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e970a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column text\n",
    "big_twitter_df = big_twitter_df.drop(['user_id','reply_count','quote_count','likes_count','retweet_counts','hyperlink'], axis=1)\n",
    "big_twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4794ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwordlist = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00670d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and removing the above stop words list from the tweet text\n",
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "big_twitter_df['cleaned'] = big_twitter_df['cleaned'].apply(lambda text: cleaning_stopwords(text))\n",
    "big_twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e0f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting tokenization of tweet text\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "big_twitter_df['cleaned'] = big_twitter_df['cleaned'].apply(tokenizer.tokenize)\n",
    "big_twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f224ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Stemming\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "big_twitter_df['cleaned'] = big_twitter_df['cleaned'].apply(lambda x: stemming_on_text(x))\n",
    "big_twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33004f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Lemmatizer\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "big_twitter_df['cleaned'] = big_twitter_df['cleaned'].apply(lambda x: lemmatizer_on_text(x))\n",
    "big_twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a338c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing words with less frequency\n",
    "# filter function to select only the words with more than 10 counts and less than 800.\n",
    "import itertools\n",
    "flat_list = list(itertools.chain.from_iterable(big_twitter_df['cleaned']))\n",
    "\n",
    "fd = nltk.FreqDist(flat_list)\n",
    "word_to_keep = list(filter(lambda x: 800>x[1]>10, fd.items()))\n",
    "\n",
    "word_list_to_keep= [item[0] for item in word_to_keep]\n",
    "\n",
    "def remove_lessfreq(tokanized_tweets):\n",
    "    text_out = [word for word in tokanized_tweets if word in word_list_to_keep]\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2cc6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_twitter_df['cleaned'] = big_twitter_df['cleaned'].apply(lambda x: remove_lessfreq(x))\n",
    "big_twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd46412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=big_twitter_df[\"cleaned\"]\n",
    "X_new  = vectorizer.transform(predict.apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5607aef",
   "metadata": {},
   "source": [
    "## Balanced Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5344b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_pred = brf_model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7882fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01292d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_twitter_df['sentiment']=new_data_pred\n",
    "big_twitter_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f94317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_twitter_df.to_csv('big_data_prediction_ml_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc01e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "sentiment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
